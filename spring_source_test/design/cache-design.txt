===============DISTRIBUTED CACHE==================
DB -> REMOTE CACHE -> LOCAL CACHE

雪崩:
REMOTE CACHE失效时间进行随机离散
穿透:
REMOTE CACHE没有命中时返回NULL(必须全量加载到缓存)
数据量大时使用bloom filter过滤
热点(击穿):
大key拆分(适用于写大于读的场景):
热点A，那么可以将A拆分为A1，A2...A100
读副本冗余(适用于读大于写的场景):
对多个slave进行读操作负载均衡
本地缓存 (适用于读大于写的场景):
1.本地缓存(防止远程缓存击穿) + 加锁更新(防止DB击穿):
过期时间短，TTL = min(REMOTE CACHE TTL，1s)
eviction策略使用LRU/LFU，保证热点key可以常驻
本地缓存过期更新本地缓存(高并发，非阻塞):
    设置本地缓存更新的触发时间(CAS), 发起一条线程去更新本地缓存(可以使用分布式锁优化,避免多机并发)
    并发线程判断更新时间是否为0，不为0则直接返回过期数据不阻塞本地线程("永不过期"，需要额外保存过期时间)
    线程判断更新时间，如果经过一定时间还没有更新，则发起另一条线程去更新本地缓存, 长时间没有更新则报错(稳定性设计)
缓存更新线程,需要先读取remote cache，然后再读DB，然后更新remote cache和本地缓存，期间需要使用分布锁保证读和写是原子操作(分布式锁太重，可以考虑版本号乐观锁)
2.中间proxy层统计热点key，并返回给client，client使用LRU策略保存起来(tair热点实现)

private int ttl;
private volatile long updateStamp;
private int updateInterval;

private Object key;
private volatile Object value;

remote cache:
缓存模式:
    cache aside, r/w through, write back
缓存一致性(读和写必须是原子的，使用版本号乐观锁保证):
    周期性从DB加载数据
    监听DB的bin log实时更新
===============DISTRIBUTED CACHE==================

===============DISTRIBUTED LOCK==================
问题:
没设置超时，加锁机器down: 设置超时时间
超时不合理，锁超时时间过短: 开启后台线程进行续期(redisson默认会设置缺省超时时间，并且起timeout任务在缺省超时时间到期前去续期)
设置合理超时，机器假死后恢复: 更新操作时注意锁的归属机器
设计:
加锁的key一定要有唯一的value，避免无法分辨锁属于哪个机器，不属于机器自己的锁无法操作
本地锁 + REMOTE锁 结合,避免本地所有线程都需要访问远程数据
没有获得锁的线程进行休眠，如果远程cache支持PUB/SUB，还可以提前唤醒线程(redisson的锁实现，这样做过于复杂)
===============DISTRIBUTED LOCK==================