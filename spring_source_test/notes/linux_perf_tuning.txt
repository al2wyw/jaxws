如何学习Linux性能优化？(有思维导图)
性能问题的本质，就是系统资源已经达到瓶颈，但请求的处理却还不够快，无法支撑更多的请求，性能分析，其实就是找出应用或系统的瓶颈，并设法去避免或者缓解它们，从而更高效地利用系统资源处理更多的请求

cpu性能篇:
平均负载是指单位时间内，系统处于可运行状态和不可中断状态的平均进程数，也就是平均活跃进程数:
所谓可运行状态的进程，是指正在使用 CPU 或者正在等待 CPU 的进程, 也就是我们在 ps 命令中看到的处于 R 状态（Running 或 Runnable）的进程;
不可中断状态的进程则是正处于内核态关键流程中的进程，并且这些流程是不可打断的，比如最常见的是等待硬件设备的 I/O 响应，也就是我们在 ps 命令中看到的 D 状态（Uninterruptible Sleep，也称为 Disk Sleep）的进程;
当一个进程向磁盘读写数据时，为了保证数据的一致性，在得到磁盘回复前，它是不能被其他进程或者中断打断的，这个时候的进程就处于不可中断状态;

平均负载为多少时合理:
当平均负载高于 CPU 数量 70% 的时候: 当只有1个cpu，此时平均负载为1.7
平均负载与 CPU 使用率:
CPU 密集型进程，使用大量 CPU 会导致平均负载升高，此时这两者是一致的； stress --cpu 1 --timeout 600 (一个CPU 使用率100%)
I/O 密集型进程，等待 I/O 也会导致平均负载升高，但 CPU 使用率不一定很高； stress -i 1 --timeout 600 (IO不停地执行sync) / stress -i 1 --hdd 1 --timeout 600 (iowait比较明显，-i的含义还是调用sync，而—hdd则表示读写临时文件)
watch -d uptime

上下文切换排查:
vmstat 1(1是用于收集样本的时间):
cs（context switch）是每秒上下文切换的次数。
in（interrupt）则是每秒中断的次数。
r（Running or Runnable）是就绪队列的长度，也就是正在运行和等待 CPU 的进程数。
b（Blocked）则是处于不可中断睡眠状态的进程数。
pidstat -wt: (t显示线程信息)
一个是cswch，表示每秒自愿上下文切换（voluntary context switches）的次数，另一个则是nvcswch，表示每秒非自愿上下文切换（non voluntary context switches）的次数, 非自愿上下文切换，则是指进程由于时间片已到等原因，被系统强制调度，进而发生的上下文切换。
上下文切换频率是多少时合理：
如果系统的上下文切换次数比较稳定，那么从数百到一万以内，都应该算是正常的。但当上下文切换次数超过一万次，或者切换次数出现数量级的增长时，就很可能已经出现了性能问题:
自愿上下文切换变多了，说明进程都在等待资源
非自愿上下文切换变多了，说明进程都在被强制调度，也就是都在争抢CPU
中断次数变多了，说明 CPU 被中断处理程序占用，还需要通过查看 /proc/interrupts 文件来分析具体的中断类型(watch -d cat /proc/interrupts)

CPU使用率高排查(瞬时进程的排查方法，无法用top,pidstat发现):
perf top -g -p pid / perf record -g -p pid/ perf report

CPU的iowait排查:
top: 进程状态 R D（Uninterruptible Sleep) Z S(interruptible Sleep) I(空闲)
查看进程的IO性能: pidstat -d / iotop   查看设备dev的IO性能: iostat
strace -p 查询进程的系统调用 / perf record -g -p记录进程的调用栈 (需要熟悉系统函数名称) !!!
iowait 高不一定代表 I/O 有性能瓶颈。当系统中只有 I/O 类型的进程在运行时，iowait 也会很高，但实际上，磁盘的读写远没有达到性能瓶颈的程度。
机械磁盘（HDD）、低端固态磁盘（SSD）与高端固态磁盘相比, 性能差异可能达到数倍到数十倍, 所以要根据实际情况调整压测的参数

CPU的中断排查:
中断其实是一种异步的事件处理机制，可以提高系统的并发处理能力:
上半部直接处理硬件请求，也就是我们常说的硬中断，它在中断禁止模式下运行，特点是快速执行，/proc/interrupts 提供了硬中断的运行情况
下半部则是由内核触发，也就是我们常说的软中断，特点是延迟执行， /proc/softirqs 提供了软中断的运行情况
每个CPU都对应一个软中断内核线程，名字为 “ksoftirqd/CPU编号”
网络请求频繁导致si软中断过高:
top查看不出异常，si只有个位数值，只能通过watch -d cat /proc/softirqs查看软中断的变化情况：TIMER（定时中断）、NET_RX（网络接收）、SCHED（内核调度）、RCU（RCU 锁）, 然后查看网络包情况 sar -n DEV 1，计算出平均包大小，收发包频率，最后tcpdump查看网络包详情

如何迅速分析出系统CPU的瓶颈:
CPU 性能指标:
第一个，CPU 使用率
第二个，应该是平均负载
第三个，进程上下文切换
最后一个，CPU 缓存的命中率
性能工具：(跳过 !!!)
如何迅速分析 CPU 的性能瓶颈：(分析推导图)
从 top 的输出可以得到各种 CPU 使用率以及僵尸进程和平均负载等信息。
从 vmstat 的输出可以得到上下文切换次数、中断次数、运行状态和不可中断状态的进程数。
从 pidstat 的输出可以得到进程的用户 CPU 使用率、系统 CPU 使用率、以及自愿上下文切换和非自愿上下文切换情况。

CPU性能优化的几个思路:
性能优化方法论:
1. 评估性能优化的效果: 1、确定性能的量化指标。(从应用程序和系统资源这两个维度，分别选择不同的指标) 2、测试优化前的性能指标。 3、测试优化后的性能指标。
2. 性能问题同时存在如何解决 (二八原则)
3. 多种优化方法的选择 (优化成本)
CPU优化的两个维度:
应用性能优化：编译器优化，算法优化，异步处理，多线程代替多进程，善用缓存
系统性能优化：CPU绑定(taskset)，CPU独占(/sys/fs/cgroup)，优先级调整，进程设置资源限制，NUMA优化，中断负载均衡
避免过早优化:
一方面，优化会带来复杂性的提升，降低可维护性；另一方面，需求不是一成不变的。针对当前情况进行的优化，很可能并不适应快速变化的新需求

内存性能篇:
在发现内存紧张时，系统就会通过一系列机制来回收内存，比如下面这三种方式:
回收缓存，比如使用 LRU（Least Recently Used）算法，回收最近使用最少的内存页面；
回收不常访问的内存，把不常用的内存通过交换分区直接写到磁盘中；
杀死进程，内存紧张时系统还会通过 OOM（Out of Memory），直接杀掉占用大量内存的进程。(/var/log/messages, dmesg)

buffer和cache，如何利用缓存提高运行性能 (跳过 !!!) 指文件在内存的缓存提效, cpu缓存可以通过perf stat来观测cache miss
如何定位内存泄露(跳过 !!!)
swap排查(跳过 !!!) 目前大部分虚拟机都不再使用交互区 top工具显示的swap total为0
如何“快准狠”找到系统内存的问题(分析推导图 跳过 !!!)
优化内存的性能：
第一种，除非有必要，Swap 应该禁止掉。这样就可以避免 Swap 的额外 I/O ，带来内存访问变慢的问题。
第二种，使用 Cgroups 等方法，为进程设置内存限制。这样就可以避免个别进程消耗过多内存，而影响了其他进程。对于核心应用，还应该降低 oom_score，避免被 OOM 杀死。
第三种，使用大页、内存池等方法，减少内存的动态分配，从而减少缺页异常。

IO性能篇:(文件系统和磁盘的性能)
文件系统 I/O分类:
第一种，根据是否利用标准库缓存，可以把文件 I/O 分为缓冲 I/O 与非缓冲 I/O
第二种，根据是否利用操作系统的页缓存，可以把文件 I/O 分为直接 I/O 与非直接 I/O O_DIRECT 标志
第三种，根据应用程序是否阻塞自身运行，可以把文件 I/O 分为阻塞 I/O 和非阻塞 I/O O_NONBLOCK 标志
第四种，根据是否等待响应结果，可以把文件 I/O 分为同步和异步 I/O O_SYNC 标志
Linux存储系统的 I/O 栈:
文件系统层，包括虚拟文件系统和其他各种文件系统的具体实现。它为上层的应用程序，提供标准的文件访问接口；对下会通过通用块层，来存储和管理磁盘数据。
通用块层，包括块设备 I/O 队列和 I/O 调度器。它会对文件系统的 I/O 请求进行排队，再通过重新排序和请求合并，然后才要发送给下一级的设备层。
设备层，包括存储设备和相应的驱动程序，负责最终物理设备的 I/O 操作。
通用块层，其实是处在文件系统和磁盘驱动中间的一个块设备抽象层。它主要有两个功能：
第一个功能跟虚拟文件系统的功能类似。向上，为文件系统和应用程序，提供访问块设备的标准接口；向下，把各种异构的磁盘设备抽象为统一的块设备，并提供统一框架来管理这些设备的驱动程序
第二个功能，通用块层还会给文件系统和应用程序发来的 I/O 请求排队，并通过重新排序、请求合并等方式，提高磁盘读写的效率(I/O 调度)。
Linux 内核支持四种 I/O 调度算法，分别是 NONE、NOOP、CFQ 以及 DeadLine。

磁盘性能指标:
使用率，是指磁盘处理 I/O 的时间百分比。过高的使用率（比如超过 80%），通常意味着磁盘 I/O 存在性能瓶颈。
饱和度，是指磁盘处理 I/O 的繁忙程度。过高的饱和度，意味着磁盘存在严重的性能瓶颈。当饱和度为 100% 时，磁盘无法接受新的 I/O 请求。
IOPS（Input/Output Per Second），是指每秒的 I/O 请求数。
吞吐量，是指每秒的 I/O 请求大小。
响应时间，是指 I/O 请求从发出到收到响应的间隔时间。
不要孤立地去比较某一指标，而要结合读写比例、I/O 类型（随机还是连续）以及 I/O 的大小，存储类型（有无RAID以及RAID级别、本地存储还是网络存储）综合来分析:
在数据库、大量小文件等这类随机读写比较多的场景中，IOPS 更能反映系统的整体性能；而在多媒体等顺序读写较多的场景中，吞吐量才更能反映系统的整体性能
iostat:
%util ，就是磁盘 I/O 使用率；
r/s + w/s ，就是 IOPS；
rkB/s + wkB/s ，就是吞吐量；
r_await + w_await ，就是响应时间ms;

狂打日志导致iowait高排查(案例有点偏激，循环打印300mb内容):
iostat -x -d 1 -> pidstat -d -> strace -f -p pid (不一定需要) -> lsof -p pid (运行多几次，因为运行时可能目标文件刚好关闭)
磁盘I/O延迟很高排查(频繁读写动态生成的暂时文件后又清除文件，无法使用lsof定位):
bcc工具箱filetop
mysql慢查询iowait高排查:(慢查询一般是cpu user高导致而不是iowait高导致，案例主动关闭系统cache: /proc/sys/vm/drop_caches)
Redis响应严重延迟排查:(redis配置appendfsync错误导致写频繁出现iowait高)
# -f表示跟踪子进程和子线程，-T表示显示系统调用的时长，-tt表示显示跟踪时间，-e过滤函数名
$ strace -f -T -tt -p 9085 -e fdatasync

迅速分析出系统I/O的瓶颈：
性能指标：
1、文件系统 I/O 性能指标：
存储空间的使用情况，包括容量、使用量以及剩余空间等
缓存使用情况，包括页缓存、目录项缓存、索引节点缓存以及各个具体文件系统（如 ext4、XFS 等）的缓存 (/proc/meminfo、/proc/slabinfo以及slabtop)
2、磁盘 I/O 性能指标：
磁盘 I/O 的性能指标，包括使用率，是指磁盘忙处理 I/O 请求的百分比， IOPS（包括 r/s 和 w/s）、响应时间（延迟）以及吞吐量（B/s）等
缓冲区（Buffer）也是要重点掌握的指标，它经常出现在内存和磁盘问题的分析中
性能工具：(跳过 !!!)
如何迅速分析 I/O 的性能瓶颈：(分析推导图)
1、先用 iostat/sar 发现磁盘 I/O 性能瓶颈；
2、再借助 pidstat ，定位出导致瓶颈的进程；
3、随后分析进程的 I/O 行为；
4、最后，结合应用程序的原理，分析这些 I/O 的来源。

磁盘 I/O 性能优化的几个思路：
I/O 基准测试(得到性能优化目标)： 首先应该对磁盘和文件系统进行基准测试，得到文件系统或者磁盘 I/O 的极限性能：使用fio进行包括随机读、随机写、顺序读以及顺序写测试
I/O 性能优化：
1、应用程序性能优化：
第一，可以用追加写代替随机写，减少寻址开销，加快 I/O 写的速度。
第二，可以借助缓存 I/O ，充分利用系统缓存，降低实际 I/O 的次数。
第三，可以在应用程序内部构建自己的缓存，或者用 Redis这类外部缓存系统。这样，一方面，能在应用程序内部，控制缓存的数据和生命周期；另一方面，也能降低其他应用程序使用缓存对自身的影响。
第四，在需要频繁读写同一块磁盘空间时，可以用 mmap 代替 read/write，减少内存的拷贝次数。
第五，在需要同步写的场景中，尽量将写请求合并，而不是让每个请求都同步写入磁盘，即可以用 fsync() 取代 O_SYNC。
第六，在多个应用程序共享相同磁盘时，为了保证 I/O 不被某个应用完全占用，推荐你使用 cgroups 的 I/O 子系统，来限制进程 / 进程组的 IOPS 以及吞吐量。
最后，在使用 CFQ 调度器时，可以用 ionice 来调整进程的 I/O 调度优先级，特别是提高核心应用的 I/O 优先级。ionice 支持三个优先级类：Idle、Best-effort 和 Realtime。
2、文件系统性能优化：
第一，你可以根据实际负载场景的不同，选择最适合的文件系统
第二，在选好文件系统后，还可以进一步优化文件系统的配置选项，包括文件系统的特性、日志模式（如 journal、ordered、writeback）、挂载选项（如 noatime）等等。
第三，优化文件系统的缓存，比如优化脏页的刷新频率、脏页限额，以及内核回收目录项缓存和索引节点缓存的倾向等等
最后，在不需要持久化时，你还可以用内存文件系统tmpfs(/dev/shm/)，以获得更好的 I/O 性能
3、磁盘优化：
第一，最简单有效的优化方法，就是换用性能更好的磁盘，比如用 SSD 替代 HDD
第二，我们可以使用 RAID ，把多块磁盘组合成一个逻辑磁盘，构成冗余独立磁盘阵列
第三，针对磁盘和应用程序 I/O 模式的特征，我们可以选择最适合的 I/O 调度算法（SSD和虚拟机中的磁盘使用noop调度，数据库应用使用deadline调度）
第四，我们可以对应用程序的数据，进行磁盘级别的隔离。比如，我们可以为日志、数据库等 I/O 压力比较重的应用，配置单独的磁盘。
第五，在顺序读比较多的场景中，我们可以增大磁盘的预读数据(调整内核选项/sys/block/sdb/queue/read_ahead_kb)
第六，我们可以优化内核块设备 I/O 的选项，如调整磁盘队列的长度


网络性能篇:
我们通常使用带宽、吞吐量、延时、PPS（包/秒）等指标，来衡量网络的性能；相应的，你可以用 ifconfig、netstat、ss、sar、ping 等工具，来查看这些网络的性能指标

C10k C100k C1000k C10m 问题 C10k即客户请求连接数1万  假如每个请求占10byte内存 -> 连接数由机器内存，带宽限制 -> 优化IO模型 多路复用select/poll/epoll AIO
从 C10k到 C100k 其实还是基于 C10K 的这些理论，epoll 配合线程池，再加上 CPU、内存和网络接口的性能和容量提升。大部分情况下，C100K 自然就可以达到。 C1000k需要从硬件的中断处理和网络功能卸载、到网络协议栈的文件描述符数量、连接状态跟踪、缓存队列等内核的优化，再到应用程序的工作模型优化。

DNS域名解析排查
公网延迟增大，缓存过期导致要重新去上游服务器请求，或者流量高峰时 DNS 服务器性能不足等，都会导致 DNS 响应的延迟增大，此时借助 nslookup 或者 dig 的调试功能，分析 DNS 的解析过程，再配合 ping 等工具调试 DNS 服务器的延迟，从而定位出性能瓶颈。
nslookup -debug time.geekbang.org -> time nslookup time.geekbang.org -> ping -c3 time.geektime.org
dig +trace time.geekbang.org
根据 IP 地址反查域名、根据端口号反查协议名称，是很多网络工具默认的行为，而这往往会导致性能工具的工作缓慢。所以，通常，网络性能工具都会提供一个选项（比如 -n 或者 -nn），来禁止名称解析

DDos攻击: 第一种，耗尽带宽。 第二种，耗尽操作系统的资源。 第三种，消耗应用程序的运行资源。
# synFlood攻击: -S参数表示设置TCP协议的SYN（同步序列号），-p表示目的端口为80， -i u10表示每隔10微秒发送一个网络帧
$ hping3 -S -p 80 -i u10 192.168.0.30
sar -n DEV 1 查看网络包情况，通过BPS除以PPS计算包的平均大小
虽然可以调整iptables和内核参数，缓解 DDoS 带来的性能问题，却无法彻底解决它。即从无法连接服务变为服务响应慢。

网络延迟排查
ping 基于 ICMP 协议，它通过计算 ICMP 响应报文与请求报文的时间差，来获得往返延时。这个过程并不需要特殊认证，常被很多网络攻击利用，比如端口扫描工具 nmap、组包工具 hping3 等等。所以，为了避免这些问题，很多网络服务会把 ICMP 禁止掉，这时，可以用 traceroute 或 hping3 的 TCP 和 UDP 模式：
# -c表示发送3次请求，-S表示设置TCP SYN，-p表示端口号为80
$ hping3 -c 3 -S -p 80 www.baidu.com
# --tcp表示使用TCP协议，-p表示端口号，-n表示不对结果中的IP地址执行反向域名解析
$ traceroute --tcp -p 80 -n baidu.com

在发现网络延迟增大后，你可以用 traceroute、hping3、tcpdump、Wireshark、strace 等多种工具，来定位网络中的潜在问题:
使用 hping3 以及 ab 等工具，确认单次请求和并发请求情况的网络延迟是否正常。
使用 traceroute，确认路由是否正确，并查看路由中每一跳网关的延迟。
使用 tcpdump 和 Wireshark，确认网络包的收发是否正常。
使用 strace 等，观察应用程序对网络套接字的调用情况是否正常。(需要熟悉unix环境编程)

NAT问题排查
Linux 内核提供的 Netfilter 框架，允许对网络数据包进行修改（比如 NAT）和过滤（比如防火墙）。在这个基础上，iptables、ip6tables、ebtables 等工具，又提供了更易用的命令行接口，以便系统管理员配置和管理 NAT、防火墙的规则。(NAT的优化需要熟悉Netfilter的内核参数以及iptables的规则)
wikipedia的Netfilter流程图:
Linux 支持 4 种表，包括 filter（用于过滤）、nat（用于 NAT）、mangle（用于修改分组数据） 和 raw（用于原始数据包）等。
跟表（table） 一起的白色背景方框，则表示链（chain），用来管理具体的 iptables 规则。每个表中可以包含多条链，比如：
filter 表中，内置 INPUT、OUTPUT 和 FORWARD 链
nat 表中，内置 PREROUTING、POSTROUTING、OUTPUT 链 (OUTPUT只处理从本机发送出去的包)
SNAT: 把源ip地址进行修改
iptables -t nat -A POSTROUTING -s 192.168.0.0/16 -j MASQUERADE
iptables -t nat -A POSTROUTING -s 192.168.0.2 -j SNAT --to-source 100.100.100.100
DNAT: 把目标ip地址进行修改
iptables -t nat -A PREROUTING -d 100.100.100.100 -j DNAT --to-destination 192.168.0.2
记得打开内核转发 sysctl net.ipv4.ip_forward

NAT由于conntrack内核参数设置太小导致丢包 (需要分析perf调用栈，难以掌握) !!!

网络性能优化的几个思路：
确定优化目标，网络协议层的性能基准测试(每层的基准测试工具 !!! 参考"怎么评估系统的网络性能?")
网络性能工具: (跳过 !!!)
网络性能优化:
应用层:
使用长连接取代短连接，可以显著降低 TCP 建立连接的成本。在每秒请求次数较多时，这样做的效果非常明显。
使用内存等方式，来缓存不常变化的数据，可以降低网络 I/O 次数，同时加快应用程序的响应速度。
使用 Protocol Buffer 等序列化的方式，压缩网络 I/O 的数据量，可以提高应用程序的吞吐。
使用 DNS 缓存、预取、HTTPDNS 等方式，减少 DNS 解析的延迟，也可以提升网络 I/O 的整体速度。
套接字层:
增大每个套接字的缓冲区大小 net.core.optmem_max；
增大套接字接收缓冲区大小 net.core.rmem_max 和发送缓冲区大小 net.core.wmem_max；
增大 TCP 接收缓冲区大小 net.ipv4.tcp_rmem 和发送缓冲区大小 net.ipv4.tcp_wmem。
为 TCP 连接设置 TCP_NODELAY 后，就可以禁用 Nagle 算法；
为 TCP 连接开启 TCP_CORK 后，可以让小包聚合成大包后再发送（注意会阻塞小包的发送）；
使用 SO_SNDBUF 和 SO_RCVBUF ，可以分别调整套接字发送缓冲区和接收缓冲区的大小。
传输层: (省略内核设置参数 !!!)
第一类，在请求数比较大的场景下，你可能会看到大量处于 TIME_WAIT 状态的连接，它们会占用大量内存和端口资源
第二类，为了缓解 SYN FLOOD 等，利用 TCP 协议特点进行攻击而引发的性能问题
第三类，在长连接的场景中，通常使用 Keepalive 来检测 TCP 连接的状态，以便对端连接断开后，可以自动回收。但是，系统默认的 Keepalive 探测间隔和重试次数，一般都无法满足应用程序的性能要求
网络层: (省略内核设置参数 !!!)
网络层，负责网络包的封装、寻址和路由，包括 IP、ICMP 等常见协议。在网络层，最主要的优化，其实就是对路由、 IP 分片以及 ICMP 等进行调优:
第一种，从路由和转发的角度出发
第二种，从分片的角度出发，最主要的是调整MTU的大小
第三种，从 ICMP 的角度出发，为了避免 ICMP 主机探测、ICMP Flood 等各种网络问题
链路层: (各种选项设置 !!!)
从应用程序、套接字、传输层、网络层以及链路层等几个角度，分别来看网络性能优化的基本思路:
在应用层中，主要是优化 I/O 模型、工作模型以及应用层的网络协议；
在套接字层中，主要是优化套接字的缓冲区大小；
在传输层中，主要是优化 TCP 和 UDP 协议；
在网络层中，主要是优化路由、转发、分片以及 ICMP 协议；
最后，在链路层中，主要是优化网络包的收发、网络功能卸载以及网卡选项。

iftop -nNP -> lsop -i:port
ss -lnt -> Recv-Q和Send-Q

为什么应用容器化后，启动慢了很多？ cpu和内存设置了限制导致运行变慢，适当调高限制值
服务器总是时不时丢包？ 从链路层到应用层每层都有丢包的机制(案例假设网络链路和客户端没有问题)：
在两台 VM 连接之间，可能会发生传输失败的错误，比如网络拥塞、线路错误等；
在网卡收包后，环形缓冲区可能会因为溢出而丢包； netstat -i RX-DRP、RX-OVR (环形缓冲区丢包)
在链路层，可能会因为网络帧校验失败、QoS 等而丢包； tc -s qdisc show dev eth1 (QoS丢包)
在 IP 层，可能会因为路由失败、组包大小超过 MTU 等而丢包；netstat -i RX-DRP、RX-OVR (环形缓冲区丢包)
在传输层，可能会因为端口未监听、资源占用超过内核限制等而丢包；netstat -s
在套接字层，可能会因为套接字缓冲区溢出而丢包；
在应用层，可能会因为应用程序异常而丢包；
此外，如果配置了 iptables 规则，这些网络包也可能因为 iptables 过滤规则而丢包； sysctl net.netfilter.nf_conntrack_count(max) / iptables -t filter -nvL
内核线程 CPU 利用率太高? perf分析内核线程的调用栈所生成的火焰图
$ perf script -i /root/perf.data | ./stackcollapse-perf.pl --all |  ./flamegraph.pl > ksoftirqd.svg
动态追踪如何使用？
动态追踪技术(Dynamic Tracing)，通过探针机制，来采集内核或者应用程序的运行信息，从而可以不用修改内核和应用程序的代码，就获得丰富的信息，帮你分析、定位想要排查的问题。
动态追踪技术的出现，就为这些问题提供了完美的方案：它既不需要停止服务，也不需要修改应用程序的代码；所有一切还按照原来的方式正常运行时，就可以帮你分析出问题的根源。同时，相比以往的进程级跟踪方法（比如 ptrace），动态追踪往往只会带来很小的性能损耗（通常在 5% 或者更少）。
为了追踪内核或用户空间的事件，动态追踪工具都会把用户传入的追踪处理函数，关联到被称为探针的检测点上。这些探针，实际上也就是各种动态追踪技术所依赖的事件源。动态追踪所使用的事件源，可以分为静态探针、动态探针以及硬件事件等三类：
硬件事件通常由性能监控计数器 PMC（Performance Monitoring Counter）产生，包括了各种硬件的性能情况，比如 CPU 的缓存、指令周期、分支预测等等；
静态探针，是指事先在代码中定义好，并编译到应用程序或者内核中的探针。这些探针只有在开启探测功能时，才会被执行到；
动态探针，则是指没有事先在代码中定义，但却可以在运行时动态添加的探针，比如函数的调用和返回等；
服务吞吐量下降很厉害，怎么分析？ 参考网络性能优化
分析性能问题的一般步骤： 总结系统(cpu，内存，IO，网络)和应用的问题分析方法
优化性能问题的一般方法： 总结系统(cpu，内存，IO，网络)和应用的优化思路
应用的优化：(先查看应用程序的响应时间、吞吐量以及错误率等性能指标，再着手思考优化方法)
第一，从 CPU 使用的角度来说，简化代码、优化算法、异步处理以及编译器优化等，都是常用的降低 CPU 使用率的方法，这样可以利用有限的 CPU 处理更多的请求。
第二，从数据访问的角度来说，使用缓存、写时复制、增加 I/O 尺寸等，都是常用的减少磁盘 I/O 的方法，这样可以获得更快的数据处理速度。
第三，从内存管理的角度来说，使用大页、内存池等方法，可以预先分配内存，减少内存的动态分配，从而更好地内存访问性能。
第四，从网络的角度来说，使用 I/O 多路复用、长连接代替短连接、DNS 缓存等方法，可以优化网络 I/O 并减少网络请求数，从而减少网络延时带来的性能问题。
第五，从进程的工作模型来说，异步处理、多线程或多进程等，可以充分利用每一个 CPU 的处理能力，从而提高应用程序的吞吐能力。
除此之外，你还可以使用消息队列、CDN、负载均衡等各种方法，来优化应用程序的架构，将原来单机要承担的任务，调度到多台服务器中并行处理